# Simulating Sensors (LiDAR, Depth Cameras, IMUs)

Accurate sensor simulation is paramount for developing and testing robot perception and navigation algorithms without relying on expensive physical hardware. Gazebo and Unity both offer robust capabilities for simulating a wide array of sensors commonly found on humanoid robots.

## LiDAR (Light Detection and Ranging)

LiDAR sensors provide 2D or 3D point cloud data, essential for mapping environments and obstacle avoidance. In simulation, LiDAR data is generated by casting rays into the environment and detecting intersections with simulated objects.

### Gazebo LiDAR Simulation

Gazebo provides a dedicated ROS 2 LiDAR plugin that can be attached to robot links. You configure parameters like:

-   `sensor_type`: `ray`
-   `pose`: Position and orientation relative to the parent link.
-   `horizontal/vertical_scan_resolution`: Angular resolution.
-   `range_min/max`: Minimum and maximum detection range.
-   `ros.topic`: The ROS 2 topic where point cloud data (`sensor_msgs/msg/LaserScan` or `sensor_msgs/msg/PointCloud2`) will be published.

### Example (within URDF or separate SDF)

```xml
<sensor name="laser_sensor" type="ray">
  <pose>0 0 0.1 0 0 0</pose>
  <visualize>true</visualize>
  <update_rate>10</update_rate>
  <ray>
    <scan>
      <horizontal>
        <samples>720</samples>
        <resolution>1</resolution>
        <min_angle>-3.14</min_angle>
        <max_angle>3.14</max_angle>
      </horizontal>
    </scan>
    <range>
      <min>0.1</min>
      <max>10.0</max>
      <resolution>0.01</resolution>
    </range>
  </ray>
  <plugin name="laser_controller" filename="libgazebo_ros_laser.so">
    <topicName>scan</topicName>
    <frameName>laser_link</frameName>
  </plugin>
</sensor>
```

## Depth Cameras

Depth cameras (like Intel RealSense or Azure Kinect) provide both color (RGB) and depth information, crucial for 3D perception, object recognition, and manipulation. Simulated depth cameras render a depth map where pixel values represent the distance to surfaces.

### Gazebo Depth Camera Simulation

Gazebo also offers camera plugins that can be configured to output depth images.

-   `image_size`: Resolution of the depth image.
-   `depth_camera_output`: `depth_image`, `point_cloud`, or `both`.
-   `ros.frame_id`: Frame for the camera data.
-   `ros.rgb_topic`/`ros.depth_topic`/`ros.point_cloud_topic`: ROS 2 topics.

## IMUs (Inertial Measurement Units)

IMUs measure linear acceleration and angular velocity, providing essential data for robot localization, balance, and attitude estimation. In simulation, IMU data is derived directly from the physics engine's calculation of the robot's motion.

### Gazebo IMU Simulation

An IMU sensor can be added to any link in Gazebo via a plugin:

-   `ros.topic`: The ROS 2 topic (`sensor_msgs/msg/Imu`) for publishing data.
-   `ros.frame_id`: The frame associated with the IMU.
-   Noise parameters can be configured to simulate real-world sensor imperfections.

### Example (within URDF or separate SDF)

```xml
<sensor name="imu_sensor" type="imu">
  <always_on>true</always_on>
  <update_rate>100</update_rate>
  <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">
    <topicName>imu</topicName>
    <frameName>base_link</frameName>
  </plugin>
</sensor>
```

By strategically placing and configuring these simulated sensors, you can create rich and realistic sensor streams to develop and rigorously test your humanoid robot's perception stack.
